---
title: "CNN_FreqLayers"
output: html_document
---



```{r}
# library(tuneR, warn.conflicts = F, quietly = T) # nice functions for reading and manipulating .wav files
# library(signal, warn.conflicts = F, quietly = T) # signal processing functions
library(dplyr, warn.conflicts = F, quietly = T)
library(reshape2, warn.conflicts = F, quietly = T)
library(keras, warn.conflicts = F, quietly = T)
require(ggplot2)
setwd("/Volumes/R2D2/Users/eoh/Documents/R_projects/rainforest/")

source("code/fns.R")
```


```{r}
# max_iter = 25
# files1 <- list.files("output/processed_data_set1", pattern="data", full.names = TRUE)
files <- list.files("output/processed_data_set4", pattern="data", full.names = TRUE)
keys <- list.files("output/processed_data_set4", pattern="key", full.names = TRUE)

# readRDS(keys[i])
# long_durations <- as.character(c(12, 2, 20, 22, 23, 23.4, 6, 8, 9))
# files <- files[c(1:4,6:13,16,20:22,24)]
# files <- rev(files)
# files <- files[6:7]

# for(i in 1:length(files)){
i=1
data <- readRDS(files[i])
data <- data %>%
    group_by(unique_id, zts) %>%
    mutate(value = value - mean(value)) %>%
    ungroup()  %>%
    mutate(value = scale(value, center=TRUE)) %>%
    mutate(value = ifelse(value < 0, 0, value))
  
data <- data %>%
  group_by(unique_id) %>%
  mutate(value = value/max(value)) %>%
  ungroup()

species <- unlist(strsplit(files[i],"_"))[[5]]
# 
# unique(data1$FreqHz) == unique(data3$FreqHz)
# 
# unique(data1$zts)
# unique(data3$zts)
# head(data1)
# head(data3)
# 
# data1 %>%
#   select(unique_id, cate) %>%
#   distinct(.) %>%
#   group_by(cate) %>%
#   summarise(n=n())

```

```{r, fig.height=8, fig.width=8}
negs <- data %>%
  filter(cate == "false_positive") %>%
  select(unique_id, species_id, cate) %>% 
  distinct(.) 

pos <- data %>%
  filter(cate == "true_positive") %>%
  filter(grepl("neu", unique_id)) %>%
  select(unique_id, species_id, cate) %>% 
  distinct(.) 

toplot <- c(sample(unique(negs$unique_id), 12), sample(unique(pos$unique_id), 12))

data %>% 
  # filter(grepl("1535", unique_id)) %>%
  filter(unique_id %in% toplot) %>%
  # filter(FreqHz > 6000 & FreqHz < 8000) %>%
  group_by(unique_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  ggplot(aes(x = zts, y = FreqHz, col = value)) +
  geom_tile() + 
  scale_color_gradient2(low = ("white"), mid = "white", high = ("red")) +
  facet_wrap(cate~unique_id, nrow=6) + theme_bw()
```

```{r, fig.height=2, fig.width=13}

data %>% 
  filter(grepl("3a129", unique_id)) %>%
  # filter(unique_id %in% toplot) %>%
  # filter(FreqHz > 6000 & FreqHz < 8000) %>%
  group_by(unique_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  ggplot(aes(x = zts, y = FreqHz, col = value^2)) +
  geom_tile() + 
  scale_color_gradient2(low = ("white"), mid = "white", high = ("red")) +
  facet_wrap(cate~unique_id, nrow=1) + theme_bw()
```

```{r}

keras_prepper_v2 <- function(input_data){
  tmp <- input_data  %>%
    mutate(cat1 = ifelse(cate == "true_positive", 1, 0),
           cat2 = ifelse(cate == "false_positive", 1, 0)) %>%
    group_by(unique_id) %>%
    mutate(zts = seqt - min(seqt)) %>%
    ungroup() %>%
    select(unique_id, cat1, cat2, FreqHz, value, zts)
  
  freq_bin <- data.frame(FreqHz = unique(tmp$FreqHz), 
                         FreqBin1 = cut(unique(tmp$FreqHz), 4, labels = c(1,1,2,2)),
                         FreqBin2 = cut(unique(tmp$FreqHz), 4, labels = c(0,3,3,0)))

  
  f1 <- tmp %>%
    left_join(., freq_bin, by="FreqHz") %>%
    dplyr::filter(FreqBin1 == 1) %>%
    dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 
  
  f2 <- tmp %>%
    left_join(., freq_bin, by="FreqHz") %>%
    dplyr::filter(FreqBin1 == 2) %>%
    dcast(unique_id + cat1 + cat2 + zts ~ FreqHz)
  
  f3 <- tmp %>%
    left_join(., freq_bin, by="FreqHz") %>%
    dplyr::filter(FreqBin2 == 3) %>%
    dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 
  
  lister <- function(x){
    list(f1 %>% dplyr::filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
         f2 %>% dplyr::filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
         f3 %>% dplyr::filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix())
  }
  
  times <- unique(tmp$zts)
  train <- lapply(1:length(times), function(x) lister(x))
  output_x <- array(c(as.numeric(unlist(train))), dim=c(dim(train[[1]][[1]]), length(train), length(train[[1]])))
  output_x_lab <- f1 %>% dplyr::filter(zts == 0) %>% select(cat2) %>% as.matrix() %>% keras::to_categorical() 
  return(list(output_x, output_x_lab))
}
```

```{r}
# save_data <- data
# data <- data %>% filter(grepl(paste(c("neu","m05","p05","m1","p1"), collapse="|"), unique_id))

max_iter = 5

  #### Model iterations
  iter_list <- vector('list', max_iter)
  j=1
  
  repeat {
    print(paste(species," ____ ", j))
    
    pos_ids <- (unique(data[data$cate == "true_positive",]$unique_id))
    neg_ids <- (unique(data[data$cate == "false_positive",]$unique_id))
    
    if(length(pos_ids) > length(neg_ids)){
    pos_ids <- c(pos_ids[grepl("_neu",pos_ids)],
       sample(pos_ids[!grepl("_neu",pos_ids)], length(neg_ids) - length(pos_ids[grepl("_neu",pos_ids)])))
    }

    data_tmp <- data %>%
      filter(unique_id %in% pos_ids | unique_id %in% neg_ids) 

    data_x <- data_tmp %>%
      filter(unique_id %in%  sample(pos_ids, length(pos_ids)*0.9) | unique_id %in% sample(neg_ids, length(neg_ids)*0.9))
    
    data_y <- data_tmp %>%
      filter(!(unique_id %in% data_x$unique_id))

    tmp <- keras_prepper_v2(data_x)
    train_x <- tmp[[1]]
    train_x_lab <- tmp[[2]]
    
    tmp <- keras_prepper_v2(data_y)
    train_y <- tmp[[1]]
    train_y_lab <- tmp[[2]]
    
    train_sum <- data_x %>%
      select(unique_id, cate) %>%
      distinct() %>%
      group_by(cate) %>%
      summarise(n=n(), .groups = 'drop')
    
    test_sum <- data_y %>%
      select(unique_id, cate) %>%
      distinct() %>%
      group_by(cate) %>%
      summarise(n=n(), .groups = 'drop')
    
    model <- keras_model_sequential()
    
    model %>% 
      layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                    filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      # layer_conv_2d(filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_conv_2d(filters = 16, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_max_pooling_2d(pool_size = c(2,2) ) %>%   #--------Max Pooling
      layer_dense(units = 8, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
      layer_dropout(rate = 0.60) %>%
      layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_dropout(rate = 0.50) %>%
      layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_dropout(rate = 0.50) %>%
      layer_activation(activation = 'relu') %>%
      layer_flatten() %>%
      # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
      # layer_dense(units = 2, activation = 'sigmoid') %>%
      compile(
        loss = 'categorical_crossentropy', 
        metrics = c("accuracy"), 
        optimizer = optimizer_adam(lr=1e-4)
      )
    
    model %>%  fit(train_x, train_x_lab, batch_size = 10, epochs = 100, shuffle = TRUE, 
                   validation_split=0.20,
                   verbose=0,
                   callbacks=list(callback_early_stopping(patience=5, verbose=1)))
    
    eval_x <- model %>% evaluate(train_x, train_x_lab)
    eval_y <- model %>% evaluate(train_y, train_y_lab)
    if(j==max_iter){break}
    j = j + 1
  }


```

### Good model: train 95%, valid 95%
```{r}
# iter = 1
# repeat{

 model <- keras_model_sequential()

 model %>% 
      layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                    filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      # layer_conv_2d(filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_conv_2d(filters = 16, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_max_pooling_2d(pool_size = c(2,2) ) %>%   #--------Max Pooling
      layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_dropout(rate = 0.20) %>%
      layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_dropout(rate = 0.50) %>%
      layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_dropout(rate = 0.50) %>%
      layer_activation(activation = 'relu') %>%
      layer_flatten() %>%
      # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
      layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
      # layer_dense(units = 2, activation = 'sigmoid') %>%
      compile(
        loss = 'categorical_crossentropy',
        metrics = c("accuracy"), 
        optimizer = optimizer_adam(lr=1e-4)
      )
    
    model %>%  fit(train_x, train_x_lab, batch_size = 10, epochs = 100, shuffle = TRUE, 
                   validation_split=0.20,
                   verbose=1,
                   callbacks=list(callback_early_stopping(patience=5, verbose=1)))

  model %>% evaluate(train_x, train_x_lab)
  model %>% evaluate(train_y, train_y_lab)
  

#   if(iter == 5){break}
#   iter = iter + 1
# }


```


### Good model:87-88%
```{r}

model <- keras_model_sequential()

model %>%
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 16, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,2), activation = 'relu') %>%
  layer_batch_normalization() %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 2, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = "adam"
  )

model %>% fit(train_x, train_x_lab, batch_size = 20, epochs = 100, validation_split=0.5)

model %>% evaluate(train_x, train_x_lab)
pred <- model %>% predict_classes(train_x) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_x_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

```


```{r}

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```

### Good model: 85
```{r}

model <- keras_model_sequential()

model %>% 
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 8, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 16, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  # layer_batch_normalization() %>%
  layer_dense(units = 8, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = optimizer_adam(lr=1e-3)
  )

summary(model)

model %>% fit(train_x, train_x_lab, batch_size = 30, epochs =100, shuffle = TRUE, validation_split=0.20,
              callbacks=list(callback_early_stopping(patience=3, verbose=1)))

model %>% evaluate(train_x, train_x_lab)

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```

### Good model: train 95, valid 90-91
```{r}

model <- keras_model_sequential()

model %>% 
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  # layer_conv_2d(filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 16, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  # layer_batch_normalization() %>%
  layer_dense(units = 8, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = optimizer_adam(lr=1e-4)
  )

summary(model)

model %>% fit(train_x, train_x_lab, batch_size = 10, epochs = 100, shuffle = TRUE, validation_split=0.20,
              callbacks=list(callback_early_stopping(patience=3, verbose=1)))

model %>% evaluate(train_x, train_x_lab)

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```



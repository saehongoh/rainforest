---
title: "CNN_FreqLayers"
output: html_document
---

```{r}
library(tuneR, warn.conflicts = F, quietly = T) # nice functions for reading and manipulating .wav files
library(signal, warn.conflicts = F, quietly = T) # signal processing functions
library(dplyr, warn.conflicts = F, quietly = T)
library(ggplot2, warn.conflicts = F, quietly = T)
require(reshape2, warn.conflicts = F, quietly = T)
library(keras, warn.conflicts = F, quietly = T)
# x_files <- list.files("output/CNN", pattern="trainX", full.names = TRUE)
key <- readRDS("output/CNN_ver4/train_species_0_key.RDS")
data <- readRDS("output/CNN_ver4/train_species_0_data.RDS")

data <- data %>%
    # filter(FreqHz > 6000 & FreqHz < 8000) %>%
      group_by(unique_id, zts) %>%
      mutate(value = value - mean(value)) %>%
      ungroup()  %>%
      mutate(value = scale(value, center=TRUE)) %>%
      mutate(value = ifelse(value < 0, 0, value))
      # mutate(value = (value - min(value)) / (max(value) - min(value))) 
      # mutate(value = ifelse(value < median(data$value) - (sd(data$value)*3), 0, value)) %>%
      # mutate(value = ifelse(value > median(data$value) + (sd(data$value)*3), 0, value))

data_x <- data %>%
  filter(unique_id %in% sample(unique(data$unique_id), length(unique(data$unique_id))*0.9))

data_y <- data %>%
  filter(!(unique_id %in% data_x$unique_id))

```

```{r}
data_x %>%
  select(unique_id, cate) %>%
  distinct() %>%
  group_by(cate) %>%
  summarise(n())
```

```{r, fig.height=8, fig.width=8}
negs <- data_x %>%
  filter(cate == "false_positive") %>%
  select(unique_id, species_id, cate) %>% 
  distinct(.) 

pos <- data_x %>%
  filter(cate == "true_positive") %>%
  select(unique_id, species_id, cate) %>% 
  distinct(.) 

toplot <- c(sample(unique(negs$unique_id), 12), sample(unique(pos$unique_id), 12))

normalization_factor <- data_x %>%
  group_by(unique_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  group_by(zts, FreqHz) %>%
  summarize(total_mean = mean(value)) %>%
  ungroup()

data_x %>% 
  filter(unique_id %in% toplot) %>%
  # filter(FreqHz > 6000 & FreqHz < 8000) %>%
  group_by(unique_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  ggplot(aes(x = zts, y = FreqHz, col = value)) +
  geom_tile() + 
  scale_color_gradient2(low = ("white"), mid = "white", high = ("red")) +
  facet_wrap(cate~unique_id, nrow=6) + theme_bw()
```

```{r}
data_x %>%
  ggplot(aes(x=value)) +
  geom_histogram() +
  facet_grid(cate~.)
```

```{r}
tmp <- data_x  %>%
  mutate(cat1 = ifelse(cate == "true_positive", 1, 0),
         cat2 = ifelse(cate == "false_positive", 1, 0)) %>%
  # filter(FreqHz > 6000 & FreqHz < 8000) %>%
  group_by(unique_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  select(unique_id, cat1, cat2, FreqHz, value, zts)
# 
# randoms <- data.frame(unique_id = unique(tmp$unique_id), randomized = sample(1:length(unique(tmp$unique_id))))
# tmp <- tmp %>%
#   left_join(., randoms, by = "unique_id") %>%
#   arrange(randomized)

freq_bin <- data.frame(FreqHz = unique(tmp$FreqHz), FreqBin = cut(unique(tmp$FreqHz), 4, labels = c(1,2,3,4)))

f1 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 1) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 

f2 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 2) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz)


f3 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 3) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 

f4 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 4) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 

lister <- function(x){
      list(f1 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f2 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f3 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f4 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix())
  }

times <- unique(tmp$zts)
train <- lapply(1:length(times), function(x) lister(x))
train_x <- array(c(as.numeric(unlist(train))), dim=c(dim(train[[1]][[1]]), length(train), length(train[[1]])))
train_x_lab <- f1 %>% filter(zts == 0) %>% select(cat2) %>% as.matrix() %>% to_categorical()


```

#### Import train Y
```{r}

tmp <- data_y %>%
  mutate(cat1 = ifelse(cate == "true_positive", 1, 0),
         cat2 = ifelse(cate == "false_positive", 1, 0)) %>%
  group_by(unique_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  select(unique_id, cat1, cat2, FreqHz, value, zts) 

freq_bin <- data.frame(FreqHz = unique(tmp$FreqHz), FreqBin = cut(unique(tmp$FreqHz), 4, labels = c(1,2,3,4)))

f1 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 1) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 

f2 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 2) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 

f3 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 3) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 

f4 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 4) %>%
  dcast(unique_id + cat1 + cat2 + zts ~ FreqHz) 

lister <- function(x){
      list(f1 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f2 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f3 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f4 %>% filter(zts ==times[x])%>%select(-unique_id, -cat1, -cat2, -zts) %>% as.matrix())
  }

times <- unique(tmp$zts)
train <- lapply(1:length(times), function(x) lister(x))
train_y <- array(c(as.numeric(unlist(train))), dim=c(dim(train[[1]][[1]]), length(train), length(train[[1]])))
train_y_lab <- f1 %>% filter(zts == 0) %>% select(cat2) %>% as.matrix() %>% to_categorical()

```

### Good model: train 95%, valid 95%
```{r}

model <- keras_model_sequential()

model %>% 
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  # layer_conv_2d(filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 16, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_max_pooling_2d(pool_size = c(2,2) ) %>%   #--------Max Pooling
  layer_dense(units = 8, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.60) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = optimizer_adam(lr=1e-4)
  )

summary(model)

model %>% fit(train_x, train_x_lab, batch_size = 10, epochs = 100, shuffle = TRUE, validation_split=0.20,
              callbacks=list(callback_early_stopping(patience=3, verbose=1)))

model %>% evaluate(train_x, train_x_lab)

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

model %>% save_model_hdf5("models/model_species_0.h5")
# new_model <- load_model_hdf5("models/model_species_0.h5")
# 
# new_model %>% evaluate(train_x, train_x_lab)
# pred <- new_model %>% predict_classes(train_y) #-----Classification
# all(pred == 1)|all(pred == 0)
# res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
#   mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
#          X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
# colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
# table(res$pred_true, res$actual_true)
# length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)
```


### Good model:87-88%
```{r}

model <- keras_model_sequential()

model %>%
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 16, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,2), activation = 'relu') %>%
  layer_batch_normalization() %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 2, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = "adam"
  )

model %>% fit(train_x, train_x_lab, batch_size = 20, epochs = 100, validation_split=0.5)

model %>% evaluate(train_x, train_x_lab)
pred <- model %>% predict_classes(train_x) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_x_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

```


```{r}

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```

### Good model: 85
```{r}

model <- keras_model_sequential()

model %>% 
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 8, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 16, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  # layer_batch_normalization() %>%
  layer_dense(units = 8, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = optimizer_adam(lr=1e-3)
  )

summary(model)

model %>% fit(train_x, train_x_lab, batch_size = 30, epochs =100, shuffle = TRUE, validation_split=0.20,
              callbacks=list(callback_early_stopping(patience=3, verbose=1)))

model %>% evaluate(train_x, train_x_lab)

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```

### Good model: train 95, valid 90-91
```{r}

model <- keras_model_sequential()

model %>% 
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  # layer_conv_2d(filters = 8, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 16, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  # layer_batch_normalization() %>%
  layer_dense(units = 8, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(rate = 0.50) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = optimizer_adam(lr=1e-4)
  )

summary(model)

model %>% fit(train_x, train_x_lab, batch_size = 10, epochs = 100, shuffle = TRUE, validation_split=0.20,
              callbacks=list(callback_early_stopping(patience=3, verbose=1)))

model %>% evaluate(train_x, train_x_lab)

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```



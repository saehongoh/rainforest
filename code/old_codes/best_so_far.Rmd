---
title: "CNN_FreqLayers"
output: html_document
---

```{r}
library(tuneR, warn.conflicts = F, quietly = T) # nice functions for reading and manipulating .wav files
library(signal, warn.conflicts = F, quietly = T) # signal processing functions
library(dplyr, warn.conflicts = F, quietly = T)
library(ggplot2, warn.conflicts = F, quietly = T)
require(reshape2, warn.conflicts = F, quietly = T)
library(keras, warn.conflicts = F, quietly = T)

read.csv("data/train_tp.csv") %>%
  group_by(species_id) %>%
  summarise(n())
```

```{r}

species_to_test <- 1

data_x <- readRDS(paste0("output/CNN/trainX_species_",species_to_test,"_data.RDS"))
data_y <- readRDS(paste0("output/CNN/trainY_species_",species_to_test,"_data.RDS"))
pos_x <- readRDS("output/pos_x.RDS")[[2]]
pos_y <- readRDS("output/pos_y.RDS")[[2]]
neg_x <- readRDS("output/neg_x.RDS")[[2]]
neg_y <- readRDS("output/neg_y.RDS")[[2]]

neg_x <- sample(neg_x[neg_x$species_id == species_to_test,]$recording_id, nrow(pos_x[pos_x$species_id == species_to_test,]))
neg_y <- sample(neg_y[neg_y$species_id == species_to_test,]$recording_id, nrow(pos_y[pos_y$species_id == species_to_test,]))

data_x <- data_x %>% filter(cate == "true_positive" | recording_id %in% neg_x)
data_y <- data_y %>% filter(cate == "true_positive" | recording_id %in% neg_y)
train_full <- rbind(data_x, data_y)

train_full %>%
  select(recording_id, cate) %>%
  distinct() %>%
  group_by(cate) %>%
  summarise(n())

```

```{r, fig.height=8, fig.width=8}
negs <- train_full %>%
  filter(cate == "false_positive") %>%
  select(recording_id, species_id, cate) %>% 
  distinct(.) 

pos <- train_full %>%
  filter(cate == "true_positive") %>%
  select(recording_id, species_id, cate) %>% 
  distinct(.) 

toplot <- c(sample(unique(negs$recording_id), 12), sample(unique(pos$recording_id), 12))

normalization_factor <- train_full %>%
  group_by(recording_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  group_by(zts, FreqHz) %>%
  summarize(total_mean = mean(value)) %>%
  ungroup()

train_full %>% 
  filter(recording_id %in% toplot) %>%
  # filter(FreqHz > 6000 & FreqHz < 8000) %>%
  group_by(recording_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  ggplot(aes(x = zts, y = FreqHz, col = value)) +
  geom_tile() + 
  scale_color_gradient2(low = ("darkblue"), mid = "grey90", midpoint = 0.5, high = ("red")) +
  facet_wrap(cate~recording_id, nrow=6)

```

```{r}
tmp <- train_full %>%
  # filter(FreqHz > 4000 & FreqHz < 5000) %>%
  group_by(recording_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  ungroup() %>%
  select(recording_id, cat1, cat2, FreqHz, value, zts)

# tmp <- rbind(tmp, tmp, tmp, tmp, tmp)

freq_bin <- data.frame(FreqHz = unique(tmp$FreqHz), FreqBin = cut(unique(tmp$FreqHz), 4, labels = c(1,2,3,4)))

f1 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 1) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

f2 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 2) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

f3 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 3) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

f4 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 4) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

lister <- function(x){
      list(f1 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f2 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f3 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f4 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix())
  }

times <- unique(tmp$zts)
train <- lapply(1:length(times), function(x) lister(x))
train_x <- array(c(as.numeric(unlist(train))), dim=c(dim(train[[1]][[1]]), length(train), length(train[[1]])))
train_x_lab <- f1 %>% filter(zts == 0) %>% select(cat2) %>% as.matrix() %>% to_categorical()

```

#### Import train Y
```{r}

tmp <- data_y %>%
  # filter(FreqHz > 4000 & FreqHz < 5000) %>%
  group_by(recording_id) %>%
  mutate(zts = seqt - min(seqt)) %>%
  select(recording_id, cat1, cat2, FreqHz, value, zts) %>%
  arrange(recording_id)

freq_bin <- data.frame(FreqHz = unique(tmp$FreqHz), FreqBin = cut(unique(tmp$FreqHz), 4, labels = c(1,2,3,4)))

f1 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 1) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

f2 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 2) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

f3 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 3) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

f4 <- tmp %>%
  left_join(., freq_bin, by="FreqHz") %>%
  filter(FreqBin == 4) %>%
  dcast(recording_id + cat1 + cat2 + zts ~ FreqHz) 

lister <- function(x){
      list(f1 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f2 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f3 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix(),
           f4 %>% filter(zts ==times[x])%>%select(-recording_id, -cat1, -cat2, -zts) %>% as.matrix())
  }

times <- unique(tmp$zts)
train <- lapply(1:length(times), function(x) lister(x))
train_y <- array(c(as.numeric(unlist(train))), dim=c(dim(train[[1]][[1]]), length(train), length(train[[1]])))
train_y_lab <- f1 %>% filter(zts == 0) %>% select(cat2) %>% as.matrix() %>% to_categorical()

```

### Good model:87-88%
```{r}

model <- keras_model_sequential()

model %>%
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 16, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,2), activation = 'relu') %>%
  layer_batch_normalization() %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 2, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = "adam"
  )

model %>% fit(train_x, train_x_lab, batch_size = 20, epochs = 100, validation_split=0.5)

model %>% evaluate(train_x, train_x_lab)
pred <- model %>% predict_classes(train_x) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_x_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

```


```{r}

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```

### Good model: 96%???
```{r}

model <- keras_model_sequential()

model %>%
  layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 16, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2), activation = 'relu') %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,2), activation = 'relu') %>%
  layer_batch_normalization() %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_dense(units = 2, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.20) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = optimizer_adam(lr=1e-4)
  )

summary(model)

model %>% fit(train_x, train_x_lab, batch_size = 40, epochs = 100, shuffle = TRUE, validation_split=0.20)

model %>% evaluate(train_x, train_x_lab)

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```

### Good model: 98???
```{r}

model <- keras_model_sequential()

model %>% layer_conv_2d(input_shape = c(dim(train_x)[-1]),
                filters = 8, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 16, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2), activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.30) %>%
  layer_dense(units = 4, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.30) %>%
  layer_dense(units = 2, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_activation(activation = 'relu') %>%
  layer_flatten() %>%
  # layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 2, activation = 'sigmoid',  kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  # layer_dense(units = 2, activation = 'sigmoid') %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c("accuracy"), 
    optimizer = optimizer_adam(lr=1e-4)
  )

summary(model)

model %>% fit(train_x, train_x_lab, batch_size = 40, epochs = 100, shuffle = TRUE, validation_split=0.20)

model %>% evaluate(train_x, train_x_lab)

pred <- model %>% predict_classes(train_y) #-----Classification
all(pred == 1)|all(pred == 0)
res <- data.frame(pred %>% to_categorical(), train_y_lab) %>%
  mutate(X1 = ifelse(X1 ==1, "pred_true","pred_false"),
         X1.1 = ifelse(X1.1 == 1, "actual_true","actual_false"))
colnames(res) <-  c("pred_true", "pred_false","actual_true","actual_false")
table(res$pred_true, res$actual_true)

length(which(pred == train_y_lab[,2]))/nrow(train_y_lab)

```